#!/usr/bin/env python

import math
import random
import copy

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
from functools import reduce
from sklearn.datasets.samples_generator import make_blobs

from lib.gmm import ParameterGMMComponent
from lib.gmm import ParameterGMMComponents
from lib.gmm import GaussianMixtureModel
from lib.es import ParameterVector
from lib.es import EvolutionStrategy

from kmeans.kmeans import Kmeans

def main():
	# target image for optimization
	img_optimization = "optimization.png"

	# data parameter
	N = 500
	D = 2
	C = 3

	samples = generate_data(N, D, C)
	df = pd.DataFrame.from_records(samples, columns=["X", "Y"])

	# K components
	K_min = 2
	K_max = 10
	K_range = range(K_min, K_max+1)

	# ES parameter
	mu = 1
	rho = 1
	lamd = 5
	sigma = 0.05

	# default ES in order to print it to console
	es = EvolutionStrategy(
		samples,
		mu,
		rho,
		lamd,
		EvolutionStrategy.TYPE_PLUS,
		[],
		None
	)
	
	print("==========================================================")
	print("# Parameter Optimization of GMMs with Evolution Strategies")
	print("==========================================================")
	print("[i] Testing Data")
	print("# Generated by multiple isophonic Gaussian Distributions")
	print(f"# > N (number of data points) = {N}")
	print(f"# > D (dimensionality of data) = {D}")
	print(f"# > C (number of components) = {C}")
	print()
	print("# The centoids and corvariances of the components are drawn")
	print("# normally distributed")
	print("==========================================================")
	print("[i] Evolution Strategy")
	print(es)
	print("==========================================================")
	print("[i] Begin optimization")
	print(f"# K (number of components) in [{K_min}, {K_max}]")
	print("==========================================================")
	
	# start optimization
	best_solutions = []
	for k in K_range:
		print(f"## K = {k}")

		# run kmeans to generate initial means
		kmeans = Kmeans(samples, k, D)
		kmeans.seed()
		kmeans.run()
		
		# plt.plot(df["X"], df["Y"], marker=".", linestyle="none")
		# plt.plot(kmeans.centers[0][0], kmeans.centers[0][1], marker="x", markersize=6)
		# plt.plot(kmeans.centers[1][0], kmeans.centers[1][1], marker="x", markersize=6)
		# plt.show()
		# return

		param_vecs = generate_param_vecs(samples, kmeans.centers, mu, k)

		es = EvolutionStrategy(
			samples,
			mu,
			rho,
			lamd,
			EvolutionStrategy.TYPE_PLUS,
			param_vecs,
			GaussianMixtureModel.loglikelihood
		)
		
		ok, validation_result = es.validate_parameter()
		if not ok:
			print(validation_result)
			return

		es.run()
		print(f"# Best solution candidate after {es.generations_cnt + 1} generations")
		print(es.best)
		
		# for generations_cnt in es.run_iter():
		# 	print(f"~~ {generations_cnt + 1} Generation")
		# 	print("# Best solution candidate")
		# 	print(es.best)
		best_solutions.append([es.best, es.generations_cnt])

		print("==========================================================")

	# plot the fitness of each ES run with individual K
	# show optimization progress
	fitnesses = list(map(lambda solution: solution[0].fitness, best_solutions))
	generation_cnts = list(map(lambda solution: solution[1], best_solutions))
	
	fig = plt.figure()
	ax = fig.add_subplot(111)
	plt.xlabel(f"K in [{K_min}, {K_max}]")
	plt.ylabel("Loglikelihood L(X | h)")
	plt.title("Optimizing GMM parameters for a given K with ES")

	ax.set_ylim([min(fitnesses) - 500, 0])
	plt.plot(K_range, fitnesses, marker="o", markersize=4)
	for i, k in enumerate(K_range):
		plt.annotate(generation_cnts[i], xy=(k, fitnesses[i] + 100))

	plt.savefig(img_optimization, bbox_inches="tight")
	print(f"[i] saved to {img_optimization}")

# generate_data creates with given parameter N data points in D-dimensional space
# each belonging to each of C isotrophic Gaussian blobs
def generate_data(N, D, C):
	variances = [abs(np.random.normal()) for _ in range(0, C)]
	
	# generate samples
	samples, _ = make_blobs(
        n_samples=N,
        n_features=D,
        centers=C,
        cluster_std=variances
    )
	return samples

# generate_param_vecs generates an initial list of parameter vector
# which within the ES are each handled as solution candidates (mu).
# Each parameter vector consists of the parameter for each k components,
# which are basically {ωi, μi, Σi}.
#
# The initial ωs are calculated as 1 / k,
# The initial μs are given by centers which were determined previously by Kmeans,
# The initial Σs are estimated with numpy over the data
def generate_param_vecs(samples, centers, mu, k):
	components = copy.deepcopy(ParameterGMMComponents())
	for i in range(0, k):
		components.append(ParameterGMMComponent(
			1 / k,
			np.asarray(centers[i]),
			np.cov(samples.T)
		))

	# create mu initial parameter vectors, which basically are the parents
	return [ParameterVector(components) for _ in range(0, mu)]

if __name__ == "__main__":
	main()