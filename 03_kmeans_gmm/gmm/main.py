#!/usr/bin/env python

import copy

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_blobs

from lib.gmm import ParameterGMMComponent
from lib.gmm import ParameterGMMComponents
from lib.gmm import GaussianMixtureModel
from lib.es import ParameterVector
from lib.es import EvolutionStrategy

from kmeans.kmeans import Kmeans

def main():
	# path for saving the optimized loglikelihoods
	# for a range of Ks
	img_optimization = "optimization.png"
	img_data = "data.png"

	# data parameter
	N = 500
	D = 2
	C = 6

	samples, labels = generate_data(N, D, C)

	# cluster data based on labels and save as picture
	clusters = [[] for i in range(0, max(labels) + 1)]
	for i in range(0, samples.shape[0]):
		clusters[labels[i]].append(samples[i])

	for cluster in clusters:
		plt.plot(np.asarray(cluster).T[0], np.asarray(cluster).T[1], marker=".", linestyle="none")

	plt.savefig(img_data)
	print(f"[i] data saved to '{img_data}'")

	# K components
	K_min = 2
	K_max = 2
	K_range = range(K_min, K_max+1)

	# ES parameter
	mu = 1
	rho = 1
	lamd = 5
	sigma = 0.05
	tau = 0.5
	min_sigma = 0.01
	max_iter = -1

	# default ES in order to print it to console
	es = EvolutionStrategy(
		samples,
		mu,
		rho,
		lamd,
		EvolutionStrategy.TYPE_PLUS,
		[],
		None,
		sigma=sigma,
		tau=tau,
		min_sigma=min_sigma,
		max_iter=max_iter
	)

	# whether to show the best solutions over a range of k's
	# or for a fixed k all solution within each generation
	show_generations = False

	print("==========================================================")
	print("# Parameter Optimization of GMMs with Evolution Strategies")
	print("==========================================================")
	print("[i] Testing Data")
	print("# Generated by multiple isophonic Gaussian Distributions")
	print(f"# > N (number of data points) = {N}")
	print(f"# > D (dimensionality of data) = {D}")
	print(f"# > C (number of components) = {C}")
	print()
	print("# The centoids and corvariances of the components are drawn")
	print("# normally distributed")
	print("==========================================================")
	print("[i] Evolution Strategy")
	print(es)
	print("==========================================================")
	print("[i] Begin optimization")
	print(f"# K (number of components) in [{K_min}, {K_max}]")
	print("==========================================================")
	
	# start optimization
	# best_solutions appends all best solutions for each k
	best_solutions = []

	# generations appends every best solution of each generations with a fixed k
	generations = []
	for k in K_range:
		print(f"## K = {k}")

		# run kmeans to generate initial means
		kmeans = Kmeans(samples, k, D)
		kmeans.seed()
		kmeans.run()
		
		# variance is the detected variance in the data divided by k
		variance = np.var(samples) / k
		param_vecs = generate_param_vecs(kmeans.centers, variance, mu, k)

		es = EvolutionStrategy(
			samples,
			mu,
			rho,
			lamd,
			EvolutionStrategy.TYPE_PLUS,
			param_vecs,
			GaussianMixtureModel.loglikelihood,
			sigma=sigma,
			tau=tau,
			min_sigma=min_sigma,
			max_iter=max_iter
		)
		
		ok, validation_result = es.validate_parameter()
		if not ok:
			print(validation_result)
			return

		if show_generations is False:
			es.run()
			
		generations = []
		if show_generations is True:
			for generations_cnt in es.run_iter():
				print(f"~~ {generations_cnt + 1}th Generation")
				print("# Best solution candidate")
				print(es.best)
				generations.append([es.best, generations_cnt])

		print(f"# Best solution candidate after {es.generations_cnt + 1} generations")
		print(es.best)
			
		best_solutions.append([es.best, es.generations_cnt])

		print("==========================================================")

	# plot the fitness of each ES run with individual K
	# show optimization progress
	if show_generations is False:
		fitnesses = list(map(lambda solution: solution[0].fitness, best_solutions))
		generation_cnts = list(map(lambda solution: solution[1], best_solutions))

		fig = plt.figure(figsize=(10, 8))
		ax = fig.add_subplot(111)
		plt.xlabel(f"K in [{K_min}, {K_max}]")
		plt.ylabel("Loglikelihood L(X | h)")
		plt.title("Optimizing GMM parameters for a given K with ES")

		ax.set_ylim([min(fitnesses) - 500, 0])
		plt.plot(K_range, fitnesses, marker="o", markersize=4)
		for i, k in enumerate(K_range):
			plt.annotate(generation_cnts[i], xy=(k, fitnesses[i] + 100))

		plt.text(0.72, 0.64, str(es).replace("\t", " " * 6), fontsize=10, transform=plt.gcf().transFigure)
		plt.grid(True)
		plt.subplots_adjust(right=0.7)

	# plot the fitness of each generation for a fixed K
	# show optimization progress
	if show_generations is True:
		fitnesses = list(map(lambda solution: solution[0].fitness, generations))
		generation = list(map(lambda solution: solution[1], generations))

		fig = plt.figure(figsize=(10, 8))
		ax = fig.add_subplot(111)
		plt.xlabel(f"K in [{K_min}, {K_max}]")
		plt.ylabel("Loglikelihood L(X | h)")
		plt.title("Optimizing GMM parameters for a given K with ES")

		ax.set_ylim([min(fitnesses) - 500, 0])
		plt.plot(generation, fitnesses, marker="o", markersize=4)

		plt.text(0.72, 0.64, str(es).replace("\t", " " * 6), fontsize=10, transform=plt.gcf().transFigure)
		plt.grid(True)
		plt.subplots_adjust(right=0.7)

	plt.savefig(img_optimization, bbox_inches="tight")
	print(f"[i] saved to '{img_optimization}'")
	print(f"[i] thank you and goodnight")

# generate_data creates with given parameter N data points in D-dimensional space
# each belonging to each of C isotrophic Gaussian blobs
def generate_data(N, D, C):
	variances = [abs(np.random.normal()) for _ in range(0, C)]

	# generate samples
	return make_blobs(
        n_samples=N,
        n_features=D,
        centers=C,
        cluster_std=variances
    )

# generate_param_vecs generates an initial list of parameter vector
# which within the ES are each handled as solution candidates (mu).
# Each parameter vector consists of the parameter for each k components,
# which are basically {ωi, μi, Σi}.
#
# The initial ωs are calculated as 1 / k,
# The initial μs are given by centers which were determined previously by Kmeans,
# The initial Σs are estimated with given variance and 0 covariance
def generate_param_vecs(centers, variance, mu, k):
	components = copy.deepcopy(ParameterGMMComponents())
	for i in range(0, k):
		components.append(ParameterGMMComponent(
			1 / k,
			np.asarray(centers[i]),
			np.asarray([[variance, 0], [0, variance]])
		))

	# create mu initial parameter vectors, which basically are the parents
	return [ParameterVector(components) for _ in range(0, mu)]

if __name__ == "__main__":
	main()